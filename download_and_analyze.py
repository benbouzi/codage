#!/usr/bin/env python3
"""
Script alternatif pour analyser le dataset Anthropic/AnthropicInterviewer
G√®re le t√©l√©chargement manuel et l'authentification Hugging Face
"""

import os
import sys
import json
from pathlib import Path
import pandas as pd

def check_local_dataset():
    """V√©rifie si le dataset existe d√©j√† localement"""
    possible_paths = [
        "anthropic_interviewer_data",
        "AnthropicInterviewer",
        "data",
        "./data"
    ]

    for path in possible_paths:
        if os.path.exists(path):
            print(f"‚úÖ Dataset trouv√© localement dans: {path}")
            return path

    return None

def download_with_token():
    """T√©l√©charge le dataset avec un token HuggingFace"""
    print("\nüîê Tentative de t√©l√©chargement avec authentification...")

    # V√©rifier si un token existe
    token = os.getenv("HF_TOKEN") or os.getenv("HUGGINGFACE_TOKEN")

    if not token:
        print("‚ùå Aucun token HuggingFace trouv√©.")
        print("\nPour t√©l√©charger le dataset automatiquement:")
        print("1. Cr√©ez un compte sur https://huggingface.co")
        print("2. Obtenez votre token: https://huggingface.co/settings/tokens")
        print("3. Exportez-le: export HF_TOKEN='votre_token'")
        return None

    try:
        from huggingface_hub import login, snapshot_download
        login(token=token)
        print("‚úÖ Authentification r√©ussie!")

        print("üì• T√©l√©chargement du dataset...")
        local_dir = snapshot_download(
            repo_id="Anthropic/AnthropicInterviewer",
            repo_type="dataset",
            local_dir="anthropic_interviewer_data"
        )
        print(f"‚úÖ Dataset t√©l√©charg√© dans: {local_dir}")
        return local_dir

    except Exception as e:
        print(f"‚ùå Erreur lors du t√©l√©chargement: {e}")
        return None

def analyze_local_data(data_path):
    """Analyse les donn√©es locales"""
    print(f"\nüìä Analyse du dataset dans: {data_path}")
    print("="*80)

    # Chercher les fichiers de donn√©es
    data_files = []
    for ext in ['.json', '.jsonl', '.csv', '.parquet']:
        data_files.extend(Path(data_path).rglob(f'*{ext}'))

    if not data_files:
        print("‚ùå Aucun fichier de donn√©es trouv√©!")
        print(f"Contenu du r√©pertoire {data_path}:")
        for item in os.listdir(data_path):
            print(f"  - {item}")
        return

    print(f"\nüìÅ Fichiers trouv√©s: {len(data_files)}")
    for f in data_files:
        print(f"  - {f}")

    # Analyser chaque fichier
    os.makedirs("output", exist_ok=True)

    for data_file in data_files:
        print(f"\n{'='*80}")
        print(f"üìÑ Analyse de: {data_file.name}")
        print(f"{'='*80}")

        try:
            # Charger les donn√©es selon le format
            if data_file.suffix == '.json':
                with open(data_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if isinstance(data, list):
                        df = pd.DataFrame(data)
                    else:
                        df = pd.DataFrame([data])

            elif data_file.suffix == '.jsonl':
                df = pd.read_json(data_file, lines=True)

            elif data_file.suffix == '.csv':
                df = pd.read_csv(data_file)

            elif data_file.suffix == '.parquet':
                df = pd.read_parquet(data_file)

            else:
                continue

            # Afficher les statistiques
            print(f"\nüìä Statistiques:")
            print(f"  - Nombre de lignes: {len(df)}")
            print(f"  - Nombre de colonnes: {len(df.columns)}")
            print(f"\nüìã Colonnes: {', '.join(df.columns)}")

            # Afficher les premi√®res lignes
            print(f"\nüëÄ Aper√ßu des donn√©es:")
            print(df.head(3).to_string())

            # Types de donn√©es
            print(f"\nüî¢ Types de donn√©es:")
            print(df.dtypes.to_string())

            # Statistiques descriptives pour les colonnes textuelles
            text_cols = df.select_dtypes(include=['object']).columns
            if len(text_cols) > 0:
                print(f"\nüìù Statistiques sur les colonnes textuelles:")
                for col in text_cols:
                    if df[col].dtype == object:
                        lengths = df[col].apply(lambda x: len(str(x)) if x else 0)
                        print(f"\n  {col}:")
                        print(f"    - Longueur moyenne: {lengths.mean():.0f} caract√®res")
                        print(f"    - Longueur m√©diane: {lengths.median():.0f} caract√®res")
                        print(f"    - Min: {lengths.min():.0f}, Max: {lengths.max():.0f}")
                        print(f"    - Valeurs uniques: {df[col].nunique()}")

            # Sauvegarder un √©chantillon
            sample_file = f"output/sample_{data_file.stem}.json"
            sample = df.head(5).to_dict(orient='records')
            with open(sample_file, 'w', encoding='utf-8') as f:
                json.dump(sample, f, indent=2, ensure_ascii=False)
            print(f"\nüíæ √âchantillon sauvegard√©: {sample_file}")

            # Sauvegarder le dataset complet en CSV
            csv_file = f"output/full_{data_file.stem}.csv"
            df.to_csv(csv_file, index=False, encoding='utf-8')
            print(f"üíæ Dataset complet export√©: {csv_file}")

        except Exception as e:
            print(f"‚ùå Erreur lors de l'analyse de {data_file.name}: {e}")
            import traceback
            traceback.print_exc()

def manual_download_instructions():
    """Affiche les instructions pour le t√©l√©chargement manuel"""
    print("\n" + "="*80)
    print("üìñ INSTRUCTIONS POUR T√âL√âCHARGEMENT MANUEL")
    print("="*80)
    print("\nEn raison du proxy, vous devez t√©l√©charger le dataset manuellement:")
    print("\n1Ô∏è‚É£  Visitez: https://huggingface.co/datasets/Anthropic/AnthropicInterviewer")
    print("2Ô∏è‚É£  Cliquez sur 'Files and versions'")
    print("3Ô∏è‚É£  T√©l√©chargez les fichiers de donn√©es (*.json, *.parquet, etc.)")
    print("4Ô∏è‚É£  Placez-les dans un dossier nomm√© 'anthropic_interviewer_data/'")
    print("5Ô∏è‚É£  Relancez ce script")
    print("\nüí° Astuce: Vous pouvez aussi utiliser wget ou curl si disponibles:")
    print("   wget https://huggingface.co/datasets/Anthropic/AnthropicInterviewer/resolve/main/data.json")
    print("\nOu utilisez un token HuggingFace:")
    print("   export HF_TOKEN='votre_token'")
    print("   python download_and_analyze.py")
    print("="*80)

def main():
    """Fonction principale"""
    print("üöÄ Analyse du dataset Anthropic/AnthropicInterviewer")
    print("="*80)

    # 1. V√©rifier si le dataset existe localement
    local_path = check_local_dataset()

    if local_path:
        analyze_local_data(local_path)
        print("\n‚úÖ Analyse termin√©e!")
        print(f"\nüìÇ R√©sultats disponibles dans le dossier 'output/'")
        return

    # 2. Essayer de t√©l√©charger avec un token
    print("\n‚ùå Dataset non trouv√© localement.")
    downloaded_path = download_with_token()

    if downloaded_path:
        analyze_local_data(downloaded_path)
        print("\n‚úÖ Analyse termin√©e!")
        return

    # 3. Afficher les instructions de t√©l√©chargement manuel
    manual_download_instructions()

if __name__ == "__main__":
    main()
