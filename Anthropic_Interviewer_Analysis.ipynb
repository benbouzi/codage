{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Analyse du Dataset Anthropic/AnthropicInterviewer\n",
    "\n",
    "Dataset contenant 1,250 transcriptions d'entretiens sur l'utilisation de l'IA au travail.\n",
    "\n",
    "**Source**: https://huggingface.co/datasets/Anthropic/AnthropicInterviewer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Installation des d√©pendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q datasets pandas matplotlib seaborn wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Chargement du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration des graphiques\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"üì• Chargement du dataset...\")\n",
    "dataset = load_dataset(\"Anthropic/AnthropicInterviewer\")\n",
    "print(\"‚úÖ Dataset charg√© avec succ√®s!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Exploration de la structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìÇ Splits disponibles:\", list(dataset.keys()))\n",
    "print(\"\\nüìä Statistiques par split:\")\n",
    "for split_name in dataset.keys():\n",
    "    print(f\"  - {split_name}: {len(dataset[split_name])} entretiens\")\n",
    "    print(f\"    Colonnes: {dataset[split_name].column_names}\")\n",
    "\n",
    "print(f\"\\nüìà Total: {sum(len(dataset[split]) for split in dataset.keys())} entretiens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üëÄ Exemples d'entretiens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher un exemple de chaque groupe\n",
    "for split_name in dataset.keys():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üìÑ EXEMPLE D'INTERVIEW ({split_name.upper()})\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    example = dataset[split_name][0]\n",
    "    \n",
    "    # Afficher toutes les cl√©s disponibles\n",
    "    for key, value in example.items():\n",
    "        if isinstance(value, str) and len(value) > 500:\n",
    "            print(f\"**{key}**: {value[:500]}...\\n\")\n",
    "        else:\n",
    "            print(f\"**{key}**: {value}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî¢ Liste des IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split_name in dataset.keys():\n",
    "    print(f\"\\nüîπ {split_name.upper()} - 10 premiers IDs:\")\n",
    "    \n",
    "    # D√©tecter le nom de la colonne d'ID\n",
    "    id_col = None\n",
    "    for col in dataset[split_name].column_names:\n",
    "        if 'id' in col.lower():\n",
    "            id_col = col\n",
    "            break\n",
    "    \n",
    "    if id_col:\n",
    "        for i in range(min(10, len(dataset[split_name]))):\n",
    "            print(f\"  - {dataset[split_name][i][id_col]}\")\n",
    "    else:\n",
    "        print(\"  (Pas de colonne ID trouv√©e)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Conversion en DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er des DataFrames pour chaque split\n",
    "dfs = {}\n",
    "\n",
    "for split_name in dataset.keys():\n",
    "    dfs[split_name] = dataset[split_name].to_pandas()\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üìã {split_name.upper()} - Aper√ßu\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(dfs[split_name].head())\n",
    "    print(f\"\\nShape: {dfs[split_name].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Statistiques d√©taill√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques globales\n",
    "total_interviews = sum(len(df) for df in dfs.values())\n",
    "\n",
    "print(f\"üìä STATISTIQUES GLOBALES\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "print(f\"Nombre total d'interviews: {total_interviews}\")\n",
    "print(f\"\\nDistribution par groupe:\")\n",
    "\n",
    "for split_name, df in dfs.items():\n",
    "    percentage = (len(df) / total_interviews) * 100\n",
    "    print(f\"  - {split_name}: {len(df)} ({percentage:.1f}%)\")\n",
    "\n",
    "# Analyse des colonnes textuelles\n",
    "text_col = None\n",
    "for col in dfs[list(dfs.keys())[0]].columns:\n",
    "    if 'text' in col.lower() or 'transcript' in col.lower():\n",
    "        text_col = col\n",
    "        break\n",
    "\n",
    "if text_col:\n",
    "    print(f\"\\nüìù Statistiques sur les transcriptions (colonne: {text_col}):\")\n",
    "    for split_name, df in dfs.items():\n",
    "        lengths = df[text_col].apply(lambda x: len(str(x)) if x else 0)\n",
    "        print(f\"\\n  {split_name}:\")\n",
    "        print(f\"    - Longueur moyenne: {lengths.mean():.0f} caract√®res\")\n",
    "        print(f\"    - M√©diane: {lengths.median():.0f} caract√®res\")\n",
    "        print(f\"    - Min: {lengths.min():.0f} caract√®res\")\n",
    "        print(f\"    - Max: {lengths.max():.0f} caract√®res\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphique de distribution par groupe\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Distribution des groupes\n",
    "group_counts = {name: len(df) for name, df in dfs.items()}\n",
    "axes[0].bar(group_counts.keys(), group_counts.values(), color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
    "axes[0].set_title('Distribution des Entretiens par Groupe', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Nombre d\\'entretiens')\n",
    "axes[0].set_xlabel('Groupe')\n",
    "\n",
    "# Camembert\n",
    "axes[1].pie(group_counts.values(), labels=group_counts.keys(), autopct='%1.1f%%', \n",
    "            colors=['#1f77b4', '#ff7f0e', '#2ca02c'], startangle=90)\n",
    "axes[1].set_title('R√©partition en %', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution des longueurs de transcriptions\n",
    "if text_col:\n",
    "    fig, ax = plt.subplots(figsize=(15, 6))\n",
    "    \n",
    "    for split_name, df in dfs.items():\n",
    "        lengths = df[text_col].apply(lambda x: len(str(x)) if x else 0)\n",
    "        ax.hist(lengths, alpha=0.6, label=split_name, bins=30)\n",
    "    \n",
    "    ax.set_title('Distribution des Longueurs de Transcriptions', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Longueur (caract√®res)')\n",
    "    ax.set_ylabel('Fr√©quence')\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Analyse de contenu (mots-cl√©s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if text_col:\n",
    "    keywords = ['AI', 'automation', 'creative', 'research', 'job', 'tool', 'help', \n",
    "                'workflow', 'efficiency', 'concern', 'future', 'learn', 'ChatGPT', \n",
    "                'Claude', 'GPT', 'assistant', 'code', 'writing', 'image']\n",
    "    \n",
    "    print(\"üîç ANALYSE DES MOTS-CL√âS\\n\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for split_name, df in dfs.items():\n",
    "        print(f\"\\nüìä {split_name.upper()}:\")\n",
    "        \n",
    "        keyword_counts = {}\n",
    "        for keyword in keywords:\n",
    "            count = df[text_col].str.contains(keyword, case=False, na=False).sum()\n",
    "            if count > 0:\n",
    "                percentage = (count / len(df)) * 100\n",
    "                keyword_counts[keyword] = (count, percentage)\n",
    "        \n",
    "        # Trier par fr√©quence\n",
    "        sorted_keywords = sorted(keyword_counts.items(), key=lambda x: x[1][0], reverse=True)\n",
    "        \n",
    "        for keyword, (count, percentage) in sorted_keywords[:10]:\n",
    "            print(f\"  - '{keyword}': {count} mentions ({percentage:.1f}% des entretiens)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚òÅÔ∏è Word Clouds par groupe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from wordcloud import WordCloud\n",
    "    \n",
    "    if text_col:\n",
    "        fig, axes = plt.subplots(1, len(dfs), figsize=(18, 5))\n",
    "        if len(dfs) == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for idx, (split_name, df) in enumerate(dfs.items()):\n",
    "            # Combiner tous les textes\n",
    "            all_text = ' '.join(df[text_col].astype(str).values)\n",
    "            \n",
    "            # Cr√©er le word cloud\n",
    "            wordcloud = WordCloud(width=800, height=400, background_color='white',\n",
    "                                colormap='viridis', max_words=100).generate(all_text)\n",
    "            \n",
    "            axes[idx].imshow(wordcloud, interpolation='bilinear')\n",
    "            axes[idx].set_title(f'Word Cloud - {split_name.upper()}', fontsize=12, fontweight='bold')\n",
    "            axes[idx].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\nexcept ImportError:\n",
    "    print(\"‚ö†Ô∏è wordcloud non install√©. Ex√©cutez: !pip install wordcloud\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Export des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporter en CSV\n",
    "for split_name, df in dfs.items():\n",
    "    filename = f\"anthropic_interviewer_{split_name}.csv\"\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"‚úÖ Export√©: {filename} ({len(df)} lignes)\")\n",
    "\n",
    "# Cr√©er un dataset combin√©\n",
    "combined_df = pd.concat([df.assign(group=name) for name, df in dfs.items()], ignore_index=True)\n",
    "combined_df.to_csv(\"anthropic_interviewer_combined.csv\", index=False)\n",
    "print(f\"‚úÖ Dataset combin√© export√©: anthropic_interviewer_combined.csv ({len(combined_df)} lignes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Rapport r√©capitulatif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = f\"\"\"\n",
    "# üìä Rapport d'Analyse - Dataset Anthropic/AnthropicInterviewer\n",
    "\n",
    "## R√©sum√©\n",
    "\n",
    "**Total d'entretiens**: {total_interviews}\n",
    "\n",
    "### Distribution par groupe\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "for split_name, df in dfs.items():\n",
    "    percentage = (len(df) / total_interviews) * 100\n",
    "    report += f\"- **{split_name}**: {len(df)} entretiens ({percentage:.1f}%)\\n\"\n",
    "\n",
    "if text_col:\n",
    "    report += f\"\\n### Statistiques sur les transcriptions\\n\\n\"\n",
    "    for split_name, df in dfs.items():\n",
    "        lengths = df[text_col].apply(lambda x: len(str(x)) if x else 0)\n",
    "        report += f\"**{split_name}**:\\n\"\n",
    "        report += f\"- Longueur moyenne: {lengths.mean():.0f} caract√®res\\n\"\n",
    "        report += f\"- M√©diane: {lengths.median():.0f} caract√®res\\n\\n\"\n",
    "\n",
    "report += f\"\"\"\n",
    "## Fichiers g√©n√©r√©s\n",
    "\n",
    "- `anthropic_interviewer_combined.csv`: Dataset complet combin√©\n",
    "\"\"\"\n",
    "\n",
    "for split_name in dfs.keys():\n",
    "    report += f\"- `anthropic_interviewer_{split_name}.csv`: Groupe {split_name}\\n\"\n",
    "\n",
    "report += f\"\"\"\n",
    "## Source\n",
    "\n",
    "Dataset: https://huggingface.co/datasets/Anthropic/AnthropicInterviewer\n",
    "\"\"\"\n",
    "\n",
    "with open(\"rapport_analyse.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(\"‚úÖ Rapport sauvegard√©: rapport_analyse.md\")\n",
    "print(\"\\n\" + report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Analyses personnalis√©es\n",
    "\n",
    "Utilisez les cellules ci-dessous pour vos propres analyses :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Votre code ici\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
